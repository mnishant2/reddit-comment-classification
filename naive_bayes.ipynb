{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import glob\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import SnowballStemmer, WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    token = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    wnl = WordNetLemmatizer()\n",
    "    ps = SnowballStemmer('english')\n",
    "    \n",
    "    tokens = token.tokenize(text.lower())\n",
    "    \n",
    "    tokens_l = [wnl.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    tokens_ls = [ps.stem(token) for token in tokens_l]\n",
    "    \n",
    "    return tokens_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('reddit-comment-classification-comp-551/reddit_train.csv')\n",
    "test = pd.read_csv('reddit-comment-classification-comp-551/reddit_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels_train = train['subreddits'].values\n",
    "x_comments_train = train['comments'].values\n",
    "printable='0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ \\n\\''\n",
    "\n",
    "x_comments_test = test['comments'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(y_labels_train)\n",
    "labels_to_index = {w: int(np.where(labels==w)[0]) for w in labels}\n",
    "index_to_labels={v:k for k,v in labels_to_index.items()}\n",
    "train['labels'] = train['subreddits'].map(labels_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_unigram = CountVectorizer(lowercase=True,\n",
    "                     stop_words=stop_words,\n",
    "                     tokenizer = preprocess,\n",
    "                    binary=True,min_df=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nishant/mvenv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "x_train_unigram = cv_unigram.fit_transform(x_comments_train)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_unigram, y, test_size=0.1, random_state=42)\n",
    "# x_train_bigram = cv_bigram.fit_transform(x_comments_train)\n",
    "# x_train_unigram_tfidf = cv_unigram_tfidf.fit_transform(x_comments_train)\n",
    "# x_train_bigram_tfidf = cv_bigram_tfidf.fit_transform(x_comments_train)\n",
    "\n",
    "\n",
    "features_unigram = cv_unigram.vocabulary_\n",
    "# features_bigram = cv_bigram.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNB(object):\n",
    "\n",
    "    def __init__(self,smoothing=True,alpha=0.1):\n",
    "        self.class_probs = None\n",
    "        self.feat_probs = None\n",
    "        self.smoothing = smoothing\n",
    "        self.alpha=alpha\n",
    "        self.class_counts=None\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train a Bernoulli naive Bayes classifier\n",
    "\n",
    "        Args:\n",
    "            X (array or sparse matrix): Each element in this array\n",
    "                is a feature vector of text\n",
    "            labels (array): The ground truth label for\n",
    "                each document\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"Compute log( P(Y) )\n",
    "        \"\"\"\n",
    "        self.class_counts = Counter(y)\n",
    "        K = len(self.class_counts)\n",
    "        denominator = float(len(y))\n",
    "        self.class_probs = np.array([log(v/denominator) for _, v in self.class_counts.items()])\n",
    "\n",
    "        \"\"\"Compute log( P(X|Y) )\n",
    "\n",
    "           Use Laplace smoothing\n",
    "           n1 + 1 / (n1 + n2 + 2)\n",
    "        \"\"\"\n",
    "        feats=X.shape[1]\n",
    "        \n",
    "        self.feat_probs = np.zeros((K,feats),dtype=float)\n",
    "       \n",
    "        # Step through each document\n",
    "        for idx,i in enumerate(y):\n",
    "            \n",
    "            for f in X[idx].indices:\n",
    "                \n",
    "                self.feat_probs[i][f] += 1.\n",
    "        \n",
    "        for i in self.class_counts.keys():\n",
    "            N = float(self.class_counts[i])\n",
    "            if self.smoothing==True:\n",
    "                self.feat_probs[i] = (self.feat_probs[i]+self.alpha)/(N +2.)\n",
    "            else:\n",
    "                self.feat_probs[i][:] = (self.feat_probs[i][:])/(N)\n",
    "            \n",
    "   \n",
    "        self.log_feat_probs = np.log(self.feat_probs)\n",
    "        self.one_minus_log_feat_probs = np.log(1 - self.feat_probs)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make a prediction from text\n",
    "        \"\"\"\n",
    "\n",
    "        y = np.zeros(X.shape[0],dtype=float)\n",
    "\n",
    "        ones = np.ones(X.shape)        \n",
    "        temp = X.dot(self.log_feat_probs.transpose()) + (ones - X).dot((self.one_minus_log_feat_probs).transpose())\n",
    "         \n",
    "        scores = temp + self.class_probs.transpose()\n",
    "        pred = np.argmax(scores, axis=1)\n",
    " \n",
    "        return np.array(pred.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=BernoulliNB()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test)\n",
    "pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5102857142857142\n"
     ]
    }
   ],
   "source": [
    "print((pred==y_test).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
